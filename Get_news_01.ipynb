{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import json\n",
    "import collections\n",
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "#from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the news link file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = json.load(open('data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2015, 3, 17)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.strptime('17/03/2015', '%d/%m/%Y').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the keys to datetime format\n",
    "links2 = {}\n",
    "for k, v in links.items():\n",
    "    k1 = datetime.datetime.strptime(k, '%d/%m/%Y').date()\n",
    "    links2[k1] = v\n",
    "\n",
    "# sort the news by date\n",
    "links2 = collections.OrderedDict(sorted(links2.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the daily column of cronica del mercado\n",
    "\n",
    "1- loop over each link, open the url and convert it to a beautifulsoup object and parse it\n",
    "\n",
    "2- loop over the soup object and extract the tags with the article body\n",
    "\n",
    "3- write the aticle into a text file and save it on the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = '/home/mahmoud/python-envs/text_analytics/Analysis/{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-21\n",
      "2018-03-06\n",
      "2018-03-07\n",
      "2018-03-08\n",
      "2018-03-09\n",
      "2018-03-12\n",
      "2018-03-13\n",
      "2018-03-14\n",
      "2018-03-15\n",
      "2018-03-16\n"
     ]
    }
   ],
   "source": [
    "for k, v in links2.items():\n",
    "    \n",
    "    # Check if the folder exists, otherwise create one\n",
    "    current_year = k.year\n",
    "    if not os.path.exists(dirpath.format(current_year)):\n",
    "        os.makedirs(dirpath.format(current_year))        \n",
    "    \n",
    "    # extract the url(s)\n",
    "    urls = v['Link']\n",
    "    try:        \n",
    "        if len(urls) == 1:\n",
    "            url = urls[0]\n",
    "            page = requests.get(url)\n",
    "            soup = bs(page.content, 'html5lib')\n",
    "            if current_year <= 2014:\n",
    "                div = soup.find('div', class_ = 'noticia primer_elemento')\n",
    "                with open(dirpath.format(current_year) + '/{}.txt'.format(k), 'a') as f:\n",
    "                    for item in div.findAll('p'):\n",
    "                        f.write(item.text + '\\n')\n",
    "            else:\n",
    "                div = soup.find('div', id = 'tamano')\n",
    "                with open(dirpath.format(current_year) + '/{}.txt'.format(k), 'a') as f:\n",
    "                    for item in div.findAll('p'):\n",
    "                        f.write(item.text + '\\n')\n",
    "        else:\n",
    "            for idx, url in enumerate(urls):\n",
    "                page = requests.get(url)\n",
    "                soup = bs(page.content, 'html5lib')\n",
    "                if current_year <= 2014:\n",
    "                    div = soup.find('div', class_ = 'noticia primer_elemento')\n",
    "                    with open(dirpath.format(current_year) + '/{}_{}.txt'.format(k, idx), 'a') as f:\n",
    "                        for item in div.findAll('p'):\n",
    "                            f.write(item.text + '\\n')\n",
    "                else:\n",
    "                    div = soup.find('div', id = 'tamano')\n",
    "                    with open(dirpath.format(current_year) + '/{}_{}.txt'.format(k, idx), 'a') as f:\n",
    "                        for item in div.findAll('p'):\n",
    "                            f.write(item.text + '\\n')\n",
    "    except:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Link': ['http://www.expansion.com/mercados/cronica-bolsa/2018/03/06/5a9e3ce646163fdf438b45cd.html'],\n",
       " 'Title': ['9. El Ibex 35 frustra a Ãºltima hora su subida']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links['06/03/2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
